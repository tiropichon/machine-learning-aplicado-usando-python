{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04cac8d",
   "metadata": {},
   "source": [
    "# Proyecto Final. Mario Ruiz Ariza.\n",
    "\n",
    "Imagina que trabajas en el departamento de marketing de una importante compañía de telefonía móvil. En los últimos meses habéis procedido a la obtención de numerosos datos relativos a diferentes modelos de teléfonos, estableciendo los precios de venta como variable principal a analizar. El planteamiento genérico del estudio que os proponéis realizar consiste en tratar de fijar un precio razonable, teniendo en cuenta el mercado, para los futuros modelos de móviles que vais a comercializar, en función de variables como el tamaño de memoria RAM, el de memoria interna, píxeles de resolución, etc.\n",
    "\n",
    "En los siguientes apartados te iré planteando una serie de cuestiones encaminadas a desarrollar este estudio aplicando diversas técnicas de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb71e5",
   "metadata": {},
   "source": [
    "<b>1. El dataset que emplearemos en este caso, al tratarse de un ejercicio de naturaleza formativa, procede de kaggle. ¿Podrías indicar de qué tipo de plataforma estamos hablando?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d32ba",
   "metadata": {},
   "source": [
    "Kaggle es una plataforma web que reune a la comunidad de Data Science más grande del mundo. En esta plataforma se puede encontrar todo tipo de  datos y código publicados por la comunidad, que pueden sernos útiles para nuestros proyectos.\n",
    "\n",
    "Kaggle te permite:\n",
    "\n",
    "1. Buscar o publicar bases de datos.\n",
    "2. Explorar y construir modelos en un espacio web con la interfaz de Jupyter Notebook.\n",
    "3. Trabajar con otros profesionales y aficionados.\n",
    "4. Realizar competencias y challenges sobre temas innovadores.\n",
    "\n",
    "Éste último punto es para mi, el más importante para los que nos estamos adentrando en el mundo de Data Science, pues nos ofrece formación, y retos para poner a prueba nuestros conocimientos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c7efe",
   "metadata": {},
   "source": [
    "<b>2. El dataset al que hacemos referencia se denomina datos_moviles.csv y puedes descargarlo al final de la descripción de este proyecto. ¿Qué tipo de archivo es este? ¿Cómo se puede cargar esta tipología de ficheros mediante la biblioteca estándar de Python?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972ffcc",
   "metadata": {},
   "source": [
    "El formato .csv corresponde a un tipo de archivo de texto en el que los datos recogidos se separan mediante comas (o puntos y comas, en los casos en los que pueda haber conflictos con números decimales). Tienen muy poco peso, y cualquier programa de hojas de cálculo puede trabajar con él, y exportarlo a otros formatos.\n",
    "\n",
    "Para cargar este tipo de archivos en Python, podemos hacerlo de dos maneras:\n",
    "\n",
    "a. A través del módulo csv. Abrimos el archivo con open() y cargamos el contenido en f, que es un objeto de archivo. Para leer, usaremos el método reader(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "nomb_fich='datos_moviles.csv'\n",
    "with open(nomb_fich) as f:\n",
    "    lectura=csv.reader(f,delimiter=';')\n",
    "    encabezados=next(lectura)\n",
    "    for fila in lectura:\n",
    "        print(fila)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ca9dc",
   "metadata": {},
   "source": [
    "b. A través de la librería Pandas, que carga los datos directamente en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "moviles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d4042",
   "metadata": {},
   "source": [
    "<b>3. Carga el fichero datos_moviles.csv en Python, de modo que los datos se guarden en un DataFrame, y escribe el código necesario para visualizar las primeras filas del dataset y para saber cuántos registros, así como el número de características que tiene.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db50703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "#Muestro las primeras 5 filas del dataset\n",
    "moviles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El dataset contiene ',len(moviles),' filas.')\n",
    "print('Está compuesto por ',len(moviles.columns),'Características.')\n",
    "print('Las características son:', moviles.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a1644",
   "metadata": {},
   "source": [
    "<b>4. En primer lugar, nos centraremos en la variable etiqueta price_range o rango de precios. Esta variable toma el valor 0 para un coste bajo del móvil, 1 para coste medio, 2 para coste alto y 3 para coste muy alto (por ejemplo, un móvil de lujo). Determina las correlaciones existentes entre todas las variables, pero, en particular, céntrate en las relaciones con price_range. ¿Cuáles son las 5 variables que tienen mayor correlación con price_range?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6131ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "print(moviles.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "columnas=['price_range','price','ram', 'battery_power','px_width','px_height']\n",
    "\n",
    "matriz_corr=np.corrcoef(moviles[columnas].values.T)\n",
    "sns.set(font_scale=1.5)\n",
    "mapa_calor=sns.heatmap(matriz_corr, cbar=True, annot=True, square=True, fmt='.2f',\n",
    "                      annot_kws={'size':15},yticklabels=columnas, xticklabels=columnas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f7bc2",
   "metadata": {},
   "source": [
    "Las 5 variables que tienen mayor correlación con la columna de price_range son, en este orden: price, ram, battery_power, px_width y px_height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13067040",
   "metadata": {},
   "source": [
    "<b>5. Dado que price (el precio en euros de cada móvil) es una variable continua, más interesante para nuestra investigación que range_price, procede a representar gráficamente la matriz de correlaciones considerando las dos variables más correlacionadas con price (excluyendo a price_range, que sirve para etiquetar los móviles en función de dicho precio): ram y battery_power. Recuerda incluir en la matriz a la propia variable price.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687fda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "#Seleccionamos las columnas que nos interesan para la matriz de correlaciones\n",
    "columnas=['price','ram','battery_power']\n",
    "\n",
    "sns.pairplot(moviles[columnas],height=2.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "#Seleccionamos las columnas que nos interesan para la matriz de correlaciones\n",
    "columnas=['price','ram','battery_power']\n",
    "\n",
    "matriz_corr=np.corrcoef(moviles[columnas].values.T)\n",
    "sns.set(font_scale=1.5)\n",
    "mapa_calor=sns.heatmap(matriz_corr, cbar=True, annot=True, square=True, fmt='.2f',\n",
    "                      annot_kws={'size':15},yticklabels=columnas, xticklabels=columnas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870614d",
   "metadata": {},
   "source": [
    "<b>6. Procede a obtener la regresión lineal de la variable price frente a la variable ram. Genera la representación gráfica, determina los coeficientes de regresión y los de determinación. ¿Se alcanza un buen ajuste?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "ram=np.array([moviles['ram']])\n",
    "ram=np.transpose(ram)\n",
    "precio=np.array(moviles['price'])\n",
    "precio=np.transpose(precio)\n",
    "\n",
    "\n",
    "#Generamos los conjuntos de entrada y de entrenamiento. Invocamos el procedimiento de\n",
    "#regresión.\n",
    "X_train, X_test, y_train, y_test=train_test_split(ram,precio)\n",
    "lr=LinearRegression().fit(X_train,y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "\n",
    "#Valores de los coeficientes.\n",
    "print('Coeficiente W1:',lr.coef_)\n",
    "print('Coeficiente W0:',lr.intercept_)\n",
    "\n",
    "#valores del coeficiente de determinación.\n",
    "print('Valor del coeficiente de determinación del conjunto de entrenamiento:',\n",
    "     round(lr.score(X_train,y_train),3))\n",
    "print('Vakir del coeficiente de determinación del conjunto de pruebas:',\n",
    "     round(lr.score(X_test,y_test),3))\n",
    "\n",
    "#Salidas gráficas.\n",
    "#Representación del conjunto de prueba.\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.xlabel('RAM')\n",
    "plt.ylabel('Precio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf5b5c",
   "metadata": {},
   "source": [
    "El coeficiente de determinación del conjunto de entrenamiento y del de prueba son muy similares, lo que revela que el ajuste no es especialmente bueno pero no hay excesivo riesgo de sobre-ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec93b1d",
   "metadata": {},
   "source": [
    "<b>7. Si quisieras fijar el precio de un móvil con 3100 GB de memoria RAM, considerando el anterior ajuste lineal, ¿qué valor establecerías?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad=[[3100]]\n",
    "\n",
    "y_pred=lr.predict(cantidad)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aaf42e",
   "metadata": {},
   "source": [
    "<b>8. Representa gráficamente los residuos obtenidos frente a los valores predichos según el modelo de regresión lineal generado (ten en cuenta que los precios de los móviles oscilan aproximadamente entre 20 y 2000 €).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51869712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "ram=np.array([moviles['ram']])\n",
    "ram=np.transpose(ram)\n",
    "precio=np.array(moviles['price'])\n",
    "precio=np.transpose(precio)\n",
    "\n",
    "#Generamos los conjuntos de entrada y de entrenamiento. Invocamos el procedimiento de\n",
    "#regresión.\n",
    "X_train, X_test, y_train, y_test=train_test_split(ram,precio)\n",
    "lr=LinearRegression().fit(X_train,y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "#Obtenemos la salida gráfica solicitada\n",
    "plt.scatter(y_train_pred, y_train_pred-y_train,c='steelblue',marker='o',edgecolor='white',\n",
    "            label='Datos de entrenamiento')\n",
    "plt.scatter(y_test_pred, y_test_pred-y_test,c='limegreen',marker='s',edgecolor='white',\n",
    "            label='Datos de prueba')\n",
    "plt.xlabel('Predicción Precios')\n",
    "plt.ylabel('Residuos')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(20,2000)\n",
    "plt.grid\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e31800",
   "metadata": {},
   "source": [
    "<b>9. Céntrate a continuación en las variables ram y battery_power, considerando price_range como una etiqueta de clasificación. Genera una clasificación del conjunto mediante un kernel lineal, incorporando, si puedes, la función plot_decisions_regions para mejorar la salida gráfica. Determina también la exactitud del test. Nota: carga los datos de price_range mediante la instrucción price_range=np.array(data['price_range']), a fin de que no tengas problemas con la dimensión de los arrays (recuerda transponerlo seguidamente).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "ram=np.array(moviles['ram'])\n",
    "ram=np.transpose(ram)\n",
    "bateria=np.array([moviles['battery_power']])\n",
    "bateria=np.transpose(bateria)\n",
    "rango=np.array(moviles['price_range'])\n",
    "rango=np.transpose(rango)\n",
    "\n",
    "X=pd.DataFrame(np.c_[bateria,ram],columns=['battery_power','ram'])\n",
    "y=rango\n",
    "\n",
    "#Creamos el conjunto de datos de entrenamiento y de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "#Realizamos el pre-escalado de características\n",
    "sc=StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "#Aplicamos una transformación para que los datos mantengan la misma media y la misma desviación estándar, guardamos\n",
    "#los datos transformados en variables con la extensión _std\n",
    "X_train_std=sc.transform(X_train)\n",
    "X_test_std=sc.transform(X_test)\n",
    "\n",
    "#Aplicamos el algoritmo LinearSVC sobre los datos normalizados\n",
    "clf=LinearSVC()\n",
    "clf.fit(X_train_std,y_train)\n",
    "\n",
    "#Analizamos finalmente la exactitud del modelo sobre los datos de prueba\n",
    "predicciones=clf.predict(X_test_std)\n",
    "accuracy=accuracy_score(y_true=y_test,y_pred=predicciones,normalize=True)\n",
    "print('')\n",
    "print(f'La exactitud des test es: {100*accuracy}%')\n",
    "\n",
    "#Creamos arrays bidimensionales con los datos del entrenamiento y del test.\n",
    "X_combined_std=np.vstack((X_train_std,X_test_std))\n",
    "y_combined=np.hstack((y_train, y_test))\n",
    "\n",
    "#Representamos el resultado con plot_decision_regions\n",
    "plt.show(plot_decision_regions(X_combined_std,y_combined,clf,legend=2,X_highlight=X_test_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04180331",
   "metadata": {},
   "source": [
    "<b>10. ¿Qué resultado obtendrías si aplicas una clasificación, en el caso anterior, de base radial gaussiana con gamma = 20?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "ram=np.array(moviles['ram'])\n",
    "ram=np.transpose(ram)\n",
    "bateria=np.array([moviles['battery_power']])\n",
    "bateria=np.transpose(bateria)\n",
    "rango=np.array(moviles['price_range'])\n",
    "rango=np.transpose(rango)\n",
    "\n",
    "X=pd.DataFrame(np.c_[bateria,ram],columns=['battery_power','ram'])\n",
    "y=rango\n",
    "\n",
    "#Creamos el conjunto de datos de entrenamiento y de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "#Realizamos el pre-escalado de características\n",
    "sc=StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "#Aplicamos una transformación para que los datos mantengan la misma media y la misma desviación estándar, guardamos\n",
    "#los datos transformados en variables con la extensión _std\n",
    "X_train_std=sc.transform(X_train)\n",
    "X_test_std=sc.transform(X_test)\n",
    "\n",
    "#Aplicamos el algoritmo SVC sobre los datos normalizados, con kernel='rbf' y gamma=20\n",
    "clf=SVC(kernel='rbf',gamma=20,C=1)\n",
    "clf.fit(X_train_std,y_train)\n",
    "\n",
    "#Analizamos finalmente la exactitud del modelo sobre los datos de prueba\n",
    "predicciones=clf.predict(X_test_std)\n",
    "accuracy=accuracy_score(y_true=y_test,y_pred=predicciones,normalize=True)\n",
    "print('')\n",
    "print(f'La exactitud des test es: {100*accuracy}%')\n",
    "\n",
    "#Creamos arrays bidimensionales con los datos del entrenamiento y del test.\n",
    "X_combined_std=np.vstack((X_train_std,X_test_std))\n",
    "y_combined=np.hstack((y_train, y_test))\n",
    "\n",
    "#Representamos el resultado con plot_decision_regions\n",
    "plt.show(plot_decision_regions(X_combined_std,y_combined,clf,X_highlight=X_test_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efbf30c",
   "metadata": {},
   "source": [
    "Con gamma=20, la exactitud del modelo ronda el 80% , pero se detecta sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685ca3a",
   "metadata": {},
   "source": [
    "<b>11. Aplica una estrategia OvR para realizar la clasificación de los datos (el Foro de trabajo 2 te proporcionará pautas para ello) y determina de nuevo la exactitud del algoritmo.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "ram=np.array(moviles['ram'])\n",
    "ram=np.transpose(ram)\n",
    "bateria=np.array([moviles['battery_power']])\n",
    "bateria=np.transpose(bateria)\n",
    "rango=np.array(moviles['price_range'])\n",
    "rango=np.transpose(rango)\n",
    "\n",
    "X=pd.DataFrame(np.c_[bateria,ram],columns=['battery_power','ram'])\n",
    "y=rango\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "sc=StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "#Aplicamos una transformación para que los datos mantengan la misma media y la misma desviación estándar, guardamos\n",
    "#los datos transformados en variables con la extensión _std\n",
    "X_train_std=sc.transform(X_train)\n",
    "X_test_std=sc.transform(X_test)\n",
    "\n",
    "#Aplicamos el algoritmo OvR sobre los datos normalizados\n",
    "ovr_clf=OneVsRestClassifier(SVC(gamma='auto'))\n",
    "ovr_clf.fit(X_train_std,y_train)\n",
    "\n",
    "#Analizamos finalmente la exactitud del modelo sobre los datos de prueba\n",
    "y_pred=ovr_clf.predict(X_test_std)\n",
    "accuracy=accuracy_score(y_true=y_test,y_pred=y_pred,normalize=True)\n",
    "print('')\n",
    "print(f'La exactitud del test es: {100*accuracy}%')\n",
    "\n",
    "#Creamos arrays bidimensionales con los datos del entrenamiento y del test.\n",
    "X_combined_std=np.vstack((X_train_std,X_test_std))\n",
    "y_combined=np.hstack((y_train, y_test))\n",
    "\n",
    "#Representamos el resultado con plot_decision_regions\n",
    "plt.show(plot_decision_regions(X_combined_std,y_combined,ovr_clf,X_highlight=X_test_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d2e00",
   "metadata": {},
   "source": [
    "<b>12. Supón ahora que no dispones del etiquetado de datos (es decir, de la variable price_range). Considerando las variables ram y price trata de obtener los posibles agrupamientos del conjunto de todos los datos mediante el algoritmo de k-medias. ¿Qué número de clústeres deberías plantear? Obtén la solución para un número de clústeres superior en una unidad. Compara los dos resultados observando las correspondientes gráficas.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03050c6f",
   "metadata": {},
   "source": [
    "K-medias con n_clusters=3 es lo que recomendaría, pues representa 3 clusters más o menos lógicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import mglearn\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "ram=np.array([moviles['ram']])\n",
    "ram=np.transpose(ram)\n",
    "precio=np.array(moviles['price'])\n",
    "precio=np.transpose(precio)\n",
    "\n",
    "X=pd.DataFrame(np.c_[ram,precio],columns=['ram','price'])\n",
    "\n",
    "#Creamos la clase KMeans y las instanciamos.\n",
    "kmeans=KMeans(n_clusters=3).fit(X)\n",
    "print(\"Los clústers a los que pertenece cada instancia son:\",kmeans.labels_)\n",
    "\n",
    "#2. Ver la relación de centroides obtenidos.\n",
    "#solo tienes que imprimir el array cluster_centers_ del objeto generado\n",
    "print('Los centroides están en:',kmeans.cluster_centers_)\n",
    "\n",
    "#3.Visualizar los resultados obtenidos.\n",
    "mglearn.discrete_scatter(X.iloc[:,0],X.iloc[:,1],kmeans.labels_,markers=\"o\")\n",
    "# Predicting the clusters\n",
    "labels = kmeans.predict(X)\n",
    "# Getting the cluster centers\n",
    "C = kmeans.cluster_centers_\n",
    "\n",
    "fig = plt.figure(figsize=(18,15))\n",
    "ax=fig.add_subplot(111,projection='3d')\n",
    "ax.scatter(X.iloc[:,0],X.iloc[:,1],c='b')\n",
    "ax.scatter(C[:,0],C[:,1],c='r', marker='*', s=1000)\n",
    "ax.set_xlabel('RAM',fontsize=15)\n",
    "ax.set_ylabel('Precio',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48ff5eb",
   "metadata": {},
   "source": [
    "k-medias con n_cluster=4. Aquí el resultado no me parece tan aceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d891621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import mglearn\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "ram=np.array([moviles['ram']])\n",
    "ram=np.transpose(ram)\n",
    "precio=np.array(moviles['price'])\n",
    "precio=np.transpose(precio)\n",
    "\n",
    "X=pd.DataFrame(np.c_[ram,precio],columns=['ram','price'])\n",
    "\n",
    "#Creamos la clase KMeans y las instanciamos.\n",
    "kmeans=KMeans(n_clusters=4).fit(X)\n",
    "print(\"Los clústers a los que pertenece cada instancia son:\",kmeans.labels_)\n",
    "\n",
    "#2. Ver la relación de centroides obtenidos.\n",
    "#solo tienes que imprimir el array cluster_centers_ del objeto generado\n",
    "print('Los centroides están en:',kmeans.cluster_centers_)\n",
    "\n",
    "#3.Visualizar los resultados obtenidos.\n",
    "mglearn.discrete_scatter(X.iloc[:,0],X.iloc[:,1],kmeans.labels_,markers=\"o\")\n",
    "# Predicting the clusters\n",
    "labels = kmeans.predict(X)\n",
    "# Getting the cluster centers\n",
    "C = kmeans.cluster_centers_\n",
    "\n",
    "fig = plt.figure(figsize=(18,15))\n",
    "ax=fig.add_subplot(111,projection='3d')\n",
    "ax.scatter(X.iloc[:,0],X.iloc[:,1],c='b')\n",
    "ax.scatter(C[:,0],C[:,1],c='r', marker='*', s=1000)\n",
    "ax.set_xlabel('RAM',fontsize=15)\n",
    "ax.set_ylabel('Precio',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d8706",
   "metadata": {},
   "source": [
    "<b>13. Obtén ahora los agrupamientos mediante el método DBSCAN y, si re resulta posible, con el método HDBSCAN. Recuerda que en el Foro de trabajo 3 ya has tratado sobre ambos métodos. Para el método DBSCAN investiga un posible valor de épsilon que proporcione un agrupamiento que te resulte razonable y para HDBSCAN emplea el recomendado por las personas que lo han desarrollado.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4cb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import mglearn\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "ram=np.array([moviles['ram']])\n",
    "ram=np.transpose(ram)\n",
    "precio=np.array(moviles['price'])\n",
    "precio=np.transpose(precio)\n",
    "\n",
    "X=pd.DataFrame(np.c_[ram,precio],columns=['ram','price'])\n",
    "\n",
    "mglearn.discrete_scatter(X.iloc[:,0],X.iloc[:,1])\n",
    "dbscan=DBSCAN(eps=3)\n",
    "dbscan.fit(X)\n",
    "mglearn.discrete_scatter(X.iloc[:,0],X.iloc[:,1],dbscan.labels_,markers=\"o\")\n",
    "\n",
    "print(dbscan.labels_)\n",
    "print(len(dbscan.core_sample_indices_))\n",
    "print(dbscan.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c45cf6",
   "metadata": {},
   "source": [
    "<b>14. Aplica el algoritmo de agrupamiento por aglomeración al conjunto de datos, considerando el número que consideres más adecuado de clústeres.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import mglearn\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "ram=np.array([moviles['ram']])\n",
    "ram=np.transpose(ram)\n",
    "precio=np.array(moviles['price'])\n",
    "precio=np.transpose(precio)\n",
    "\n",
    "X=pd.DataFrame(np.c_[ram,precio],columns=['ram','price'])\n",
    "\n",
    "#Aplicamos agrupamiento por aglomeración\n",
    "mglearn.discrete_scatter(X.iloc[:,0],X.iloc[:,1])\n",
    "agg=AgglomerativeClustering(n_clusters=2,linkage='single')\n",
    "agg.fit(X)\n",
    "mglearn.discrete_scatter(X.iloc[:,0],X.iloc[:,1],agg.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ad03c",
   "metadata": {},
   "source": [
    "<b>15. Considera ahora el dataset completo. Aplica el algoritmo PCA y obtén y representa la varianza explicada en función del número de dimensiones. ¿Cuántas dimensiones requerirás para salvaguardar una varianza en torno al 95 %?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Cargar los datos en un dataframe\n",
    "moviles=[]\n",
    "moviles=pd.read_csv('datos_moviles.csv',sep=',')\n",
    "\n",
    "moviles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fe275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos información sobre los datos\n",
    "moviles.info()\n",
    "print(\"\\nVarianza de cada dimensión:\")\n",
    "print(moviles.var(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c78527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos el algoritmo PCA\n",
    "pca_pipe = make_pipeline(StandardScaler(), PCA())\n",
    "pca_pipe.fit(moviles)\n",
    "\n",
    "#Guardamos los datos del método PCA aplicado en modelo_pca\n",
    "modelo_pca = pca_pipe.named_steps['pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos la representación gráfica de las varianzas explicadas\n",
    "cumsum = np.cumsum(modelo_pca.explained_variance_ratio_)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "ax.plot(np.arange(len(moviles.columns)) + 1,cumsum, marker = 'o')\n",
    "plt.xlabel(\"Dimensiones principales\")\n",
    "plt.ylabel(\"Varianza explicada\")\n",
    "for x, y in zip(np.arange(len(moviles.columns)) + 1, cumsum):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(label,(x,y),textcoords=\"offset points\",xytext=(0,10),ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3292c5",
   "metadata": {},
   "source": [
    "En torno a 18 dimensiones alcanzamos una varianza del 0.93, por lo que podríamos plantearnos una reducción hasta dicho número de características.\n",
    "\n",
    "La matriz de proyecciones sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a242b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos las proyecciones de la matriz original\n",
    "proyecciones = pca_pipe.transform(X=moviles)\n",
    "proyecciones = pd.DataFrame(proyecciones, columns = ['CP1','CP2','CP3','CP4','CP5','CP6','CP7','CP8','CP9','CP10','CP11','CP12','CP13','CP14','CP15','CP16','CP17','CP18','CP19','CP20','CP21','CP22'],index = moviles.index)\n",
    "print(proyecciones.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d38b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
